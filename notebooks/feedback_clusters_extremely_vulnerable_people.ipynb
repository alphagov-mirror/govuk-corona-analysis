{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:08:17.141226Z",
     "start_time": "2020-04-08T14:08:17.136629Z"
    }
   },
   "source": [
    "* identify the problem\n",
    "    * cluster feedback from extremely vulnerable peeps, extract \"themes\" - divide them into groups based on some measure of similarity\n",
    "* represent data using numeric attributes\n",
    "* use a standard algorithm to find a model\n",
    "    * manually generate labels for the top n clusters\n",
    "\n",
    "> All models are wrong some are useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T13:47:20.647928Z",
     "start_time": "2020-04-11T13:47:17.779370Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import src.utils.regex as regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "#language packages\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "# some viz, requires matplotlib\n",
    "import mpld3\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# takes a while to run\n",
    "! spacy download en_core_web_lg\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "STEMMER = PorterStemmer()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# instantiate\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "# pandas options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:22:52.842179Z",
     "start_time": "2020-04-10T14:22:52.839150Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "feedback_data_path = os.path.join(data_path, 'joined_uis_all_of_march.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:22:54.209161Z",
     "start_time": "2020-04-10T14:22:52.844804Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(feedback_data_path)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's filter for users who visit '/coronavirus-extremely-vulnerable' during their journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:22:54.213906Z",
     "start_time": "2020-04-10T14:22:54.211223Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corona_slugs = ['/coronavirus-extremely-vulnerable', '/done/coronavirus-extremely-vulnerable']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of entries contain questions about travel advice, often with individual country names\n",
    "this meant the clusterer was clustering by country name which wasn't ideal\n",
    "The same goes for months etc, so they are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:23:04.257083Z",
     "start_time": "2020-04-10T14:22:54.216205Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')\n",
    "def remove_common_terms(text):\n",
    "    doc = model(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"DATE\":\n",
    "            text = text.replace(ent.text, ent.label_)\n",
    "    return text\n",
    "\n",
    "# Sanity check\n",
    "print(remove_common_terms(\"to find out an update for my holiday in mexico in april\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data, there is a lot going on here, explained in the comments. This takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:44.300643Z",
     "start_time": "2020-04-10T14:23:04.259001Z"
    },
    "code_folding": [
     5
    ],
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "q3 = \"Q3\"\n",
    "df['q3_copy'] = df[q3]\n",
    "\n",
    "# These are terms that are functionally the same but people use different terms, this standardises them\n",
    "# to be improved..., \n",
    "same_terms = {\n",
    "    \"travelling\": \"travel\",\n",
    "    \"travellers\": \"travel\",\n",
    "    \"holiday\": \"travel\",\n",
    "    \"self-isolation\": \"quarantine\",\n",
    "    \"selfisolation\": \"quarantine\",\n",
    "    \"self isolation\": \"quarantine\",\n",
    "    \"isolation\": \"quarantine\",\n",
    "    \"statuatory sick pay\": \"ssp\",\n",
    "    \"sick pay\": \"ssp\",\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # We'll be removing non alphabetical characters but we want to keep the non emergency phone number \n",
    "    # '111' in, so we'll just replace that with text\n",
    "    text = text.replace(\"111\", \"oneoneone\")\n",
    "    # Same for 999\n",
    "    text = text.replace(\"999\", \"nineninenine\")\n",
    "    # Remove non alphabetical or space characters\n",
    "    text = re.sub(\"[^a-zA-Z\\s:]\", \"\", text)\n",
    "    # Use our function from previous cell\n",
    "    text = remove_common_terms(text)\n",
    "    # This is done after remove_common_terms because spacy doesn't \n",
    "    # always recognise country names without a capital letter at the beginning!\n",
    "    text = text.lower()\n",
    "    text = re.sub(regex.coronavirus_misspellings_and_typos_regex() + \"|virus\", \"\", text)\n",
    "    # People using different terms for \"I want to know\", so just remove those\n",
    "    text = re.sub(\"wanted to find out|to look up about|to get an update|to find infos|to find info|to find out|to understand|to read the|check on advice|to check|ti get advice|to get advice|for information on\", \"\", text)\n",
    "    for word_to_replace, word_to_replace_with in same_terms.items():\n",
    "        text.replace(word_to_replace, word_to_replace_with)\n",
    "    return text\n",
    "\n",
    "# df[q3] = df[q3].apply(clean_text) # this takes a while, progress_map let's us see progress\n",
    "df[q3] = df[q3].progress_map(clean_text)\n",
    "\n",
    "print(\"The number of rows and columns after cleaning: \", df.shape)\n",
    "# Remove rows without a page sequence\n",
    "df = df[df['PageSequence'].notnull()].reset_index(drop=True)\n",
    "print(\"The number of rows and columns after dropping nulls for PageSequence: \", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.946090Z",
     "start_time": "2020-04-10T14:31:44.302484Z"
    },
    "code_folding": [
     7
    ],
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corona_related_items_regex = regex.coronavirus_misspellings_and_typos_regex() + '|sick pay|ssp|sick|isolation|closures|quarantine|closure|cobra|cruise|hand|isolat|older people|pandemic|school|social distancing|symptoms|cases|travel|wuhan|care|elderly|care home|carehome'\n",
    "\n",
    "# We only want to cluster rows that are relevant to corona stuff\n",
    "# so we have the column 'has_corona_page'\n",
    "# It is only true if they have visted a corona page AND included a relevant term in the feedback\n",
    "# (there was some irrelevant stuff about passports, we may want to remove the need for a relevant term\n",
    "# as people may be using terms not in that list and we might miss out on some insights)\n",
    "for index, row in df.iterrows():\n",
    "    has_corona_page = False\n",
    "    # if re.search(corona_related_items_regex, df.at[index, q3]) is not None:\n",
    "    for slug in row['PageSequence'].split(\">>\"):\n",
    "        if slug in corona_slugs:\n",
    "            has_corona_page = True\n",
    "    df.at[index, 'has_corona_page'] = has_corona_page\n",
    "df = df[df['has_corona_page']].reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate users\n",
    "df = df.drop_duplicates('intents_clientID')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment / feedback clustering\n",
    "How can we learn about the underlying structure of feedback in a way that is informative and intuitive? The basic approach is to analyse the latent topics within each comment. This will require a pipeline that tokenises & stems, transforms the tokens into a vector space model (tf-idf), and then clusters them into groups (k-means or hdbscan or something).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "* tokenizing and stemming each synopsis\n",
    "* transforming the corpus into vector space using tf-idf\n",
    "* calculating cosine distance between each document as a measure of similarity\n",
    "* clustering the documents using the k-means algorithm\n",
    "* using multidimensional scaling to reduce dimensionality within the corpus\n",
    "* plotting the clustering output using matplotlib and mpld3\n",
    "* topic modeling using Latent Dirichlet Allocation (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the purposes of this walkthrough, imagine that we have 2 primary lists:\n",
    "* 'Q3': seems to be the most pertinent question,\n",
    "* 'label': some unique identifier would be helpful, but actually we primarily want something to label each comment with for humans to read.\n",
    "\n",
    "We might want to extend this to include question 8 later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T08:07:13.411432Z",
     "start_time": "2020-04-09T08:07:13.328524Z"
    }
   },
   "source": [
    "# Stopwords, stemming, and tokenizing\n",
    "This section is focused on defining some functions to manipulate the synopses. First, we load NLTK's list of English stop words. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.955063Z",
     "start_time": "2020-04-10T14:31:50.949138Z"
    }
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customise our stopwords using a feedback loop informed by running through this process. We've retrosepctively come back and extended as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.960951Z",
     "start_time": "2020-04-10T14:31:50.957415Z"
    }
   },
   "outputs": [],
   "source": [
    "# through experimenting we found some additional ones to add\n",
    "new_stopwords = [\"'d\", \"'s\", 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'onc', 'onli', 'sha', 'whi', 'wo', 'would']\n",
    "stopwords.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the Snowball Stemmer which is actually part of NLTK. Stemming is just the process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.966525Z",
     "start_time": "2020-04-10T14:31:50.962986Z"
    }
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below we define two functions:\n",
    "\n",
    "* tokenize_and_stem: tokenizes (splits the comment into a list of its respective words (or tokens) and also stems each token\n",
    "* tokenize_only: tokenizes the comment only\n",
    "We use both these functions to create a dictionary which becomes important in case we want to use stems for an algorithm, but later convert stems back to their full words for presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.975133Z",
     "start_time": "2020-04-10T14:31:50.968884Z"
    },
    "code_folding": [
     3,
     15
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# here we define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:50.983200Z",
     "start_time": "2020-04-10T14:31:50.977237Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0:10]['Q3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use my stemming/tokenizing and tokenizing functions to iterate over the list of comments to create two vocabularies: one stemmed and one only tokenized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:51.012053Z",
     "start_time": "2020-04-10T14:31:50.984905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply a user defined function to each column by doubling each value in each column\n",
    "df.iloc[0:10]['Q3'].apply(tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:51.027045Z",
     "start_time": "2020-04-10T14:31:51.014012Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0:10]['Q3'].apply(tokenize_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:52.750784Z",
     "start_time": "2020-04-10T14:31:51.028870Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's apply to the dataframe\n",
    "df['Q3_tokenized_and_stemmed'] = df['Q3'].progress_map(tokenize_and_stem)\n",
    "\n",
    "df['Q3_tokenized_only'] = df['Q3'].progress_map(tokenize_only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used stemming/tokenizing and tokenizing functions to iterate over the column of comments, from this we have created two vocabularies: one stemmed and one only tokenized. This is a slightly different approach, we take the data out of a pandas data frame format for this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:54.486630Z",
     "start_time": "2020-04-10T14:31:52.752631Z"
    }
   },
   "outputs": [],
   "source": [
    "#not super pythonic\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in df['Q3'].to_list():\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'comments list', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these two columns, we can create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column. The benefit of this is it provides an efficient way to look up a stem and return a full token. The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc. For our purposes this is fine--we should be perfectly happy returning the first token associated with the stem we need to look up, as it's for labelling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:54.505351Z",
     "start_time": "2020-04-10T14:31:54.488786Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are only 51326 items in the DataFrame which isn't huge overhead in looking up a stemmed word based on the stem-index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:54.513773Z",
     "start_time": "2020-04-10T14:31:54.507023Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vocab_frame.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf and document similarityÂ¶\n",
    "\n",
    "Here, we define term frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the comments list into a tf-idf matrix.\n",
    "\n",
    "To get a Tf-idf matrix, first count word occurrences by document / comment. This is transformed into a document-term matrix (dtm). This is also just called a term frequency matrix. \n",
    "\n",
    "Then apply the term frequency-inverse document frequency weighting: words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document.\n",
    "\n",
    "A couple things to note about the parameters we define below:\n",
    "\n",
    "* `max_df`: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than X% of the documents it probably cares little meanining \n",
    "* `min_df`: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here we pass 0.2; the term must be in at least 20% of the document.\n",
    "* `ngram_range`: this just means we'll look at unigrams, bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:55.904916Z",
     "start_time": "2020-04-10T14:31:54.515413Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=200000,\n",
    "                                 min_df=0.05, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "# we convert the pandas df to a list for consumption\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df['Q3'].to_list()) #fit the vectorizer to all comments\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terms is just a list of the features used in the tf-idf matrix. This is a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:55.910297Z",
     "start_time": "2020-04-10T14:31:55.906678Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist is defined as 1 - the cosine similarity of each document. Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus (each synopsis among the synopses). Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane.\n",
    "\n",
    "Note that with dist it is possible to evaluate the similarity of any two or more comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:31:55.971014Z",
     "start_time": "2020-04-10T14:31:55.912520Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "Using the tf-idf matrix, we can run a slew of clustering algorithms to better understand the hidden structure within the comments. We first chose k-means, as it's a good baseline. K-means initializes with a pre-determined number of clusters (this could also be considered helpful - \"What are the top n clsuters people are commenting on?\"). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:46.957793Z",
     "start_time": "2020-04-11T10:09:43.873067Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use joblib.dump to pickle the model, once it has converged and to reload the model/reassign the labels as the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:46.970217Z",
     "start_time": "2020-04-11T10:09:46.960604Z"
    }
   },
   "outputs": [],
   "source": [
    "joblib.dump(km,  '../models/doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('../models/doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:46.977831Z",
     "start_time": "2020-04-11T10:09:46.972436Z"
    }
   },
   "outputs": [],
   "source": [
    "# should be the same, where the cluster corresponds to our comment of interest\n",
    "print(df.shape)\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:46.985207Z",
     "start_time": "2020-04-11T10:09:46.979720Z"
    }
   },
   "outputs": [],
   "source": [
    "# create assocaited variable\n",
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some fancy indexing and sorting on each cluster to identify which are the top n words that are nearest to the cluster centroid. This gives a good sense of the main topic of the cluster.\n",
    "\n",
    "First we remind ourselves ofthe structure of terms found in the comments. This contains the lemma and the associated word (non-lemmatized version of the word with the same meaning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:48.642761Z",
     "start_time": "2020-04-11T10:09:48.616065Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing .loc \n",
    "ind = 4\n",
    "print(\"We are interested in this lemma found in the comments:\")\n",
    "print()\n",
    "print(terms[ind])\n",
    "print()\n",
    "print(\"We are curious about the lexeme, the set of all forms that have the same meaning\")\n",
    "print()\n",
    "print(vocab_frame.loc[terms[ind].split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:49.342710Z",
     "start_time": "2020-04-11T10:09:49.335170Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:09:50.238258Z",
     "start_time": "2020-04-11T10:09:50.180126Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:32:10.352972Z",
     "start_time": "2020-04-11T14:32:10.284595Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        # try vocab_frame.loc[terms[ind]]['words'], ix is deprecated\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "        # print(' %s' % vocab_frame.loc[terms[ind]]['words'].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d Q3:\" % i, end='')\n",
    "    for title in df.iloc[i]['Q3']:\n",
    "        print('%s' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a better idea of some of the exemplar comments in these clusters. That will help us manually generate a sensible theme or label. We should also see the proportion of comments in each cluster. We can afford this to do manually for specific subsets of journeys, in this case we are focused on extremely vulnerable people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:25:58.903974Z",
     "start_time": "2020-04-11T14:25:58.871420Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    cluster_examples = df[df.cluster.eq(i)].Q3_tokenized_only.head(5).to_numpy()\n",
    "    cluster_count = df.cluster.value_counts()[i]\n",
    "    print(f\"Some example comments from Cluster {i}: \\n\\n{cluster_examples}\\n\\n There are {cluster_count} of these examples. \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T10:44:42.522115Z",
     "start_time": "2020-04-11T10:44:42.381590Z"
    }
   },
   "outputs": [],
   "source": [
    "# easier to read\n",
    "print(df.cluster.value_counts())\n",
    "df.cluster.value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some interesting features from our SQL query from the GA data associated with the sessions associated wiht the comments, let's look at them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T13:20:09.078945Z",
     "start_time": "2020-04-11T13:20:08.981630Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summary stats to compare characteristics of groups\n",
    "\n",
    "clusterino = df.groupby(\"cluster\")\n",
    "\n",
    "# Summary statistic of all clusters\n",
    "(\n",
    "    clusterino[['dayofweek',\n",
    "                'total_seconds_in_session_across_days',\n",
    "                'total_pageviews_in_session_across_days',\n",
    "                'guidance_count',\n",
    "                'done_page_flag']]\n",
    "    .describe()\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that cluster 0, sessions spent more time on GOV.UK than the other clusters and also there was a higher mean `guidance_count`. This suggests users from this cluster did spend some time and effort looking for content or information that may or may not exist. Digging through this cluster might provide insight for content that is lacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordclouds\n",
    "Wordclouds can be useful ways to summarise a cluster, and the comments therein. WordCloud is a technique to show which words are the most frequent among the given text. The first thing you may want to do before using any functions is check out the docstring of the function, and see all required and optional arguments. To do so, type ?function and run it to get all information.\n",
    "\n",
    "Ideally we would use the same stopwords from earlier and maybe the toeknized and stemmed comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T13:59:08.933669Z",
     "start_time": "2020-04-11T13:59:08.926587Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \" \".join(comment for comment in df[df.cluster == 0].Q3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:13:10.548832Z",
     "start_time": "2020-04-11T14:13:09.910242Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    text = \" \".join(comment for comment in df[df.cluster == i].Q3)\n",
    "    comments_n = len(df[df.cluster == i].Q3)\n",
    "\n",
    "    \n",
    "    stopwords_wc = set(STOPWORDS)\n",
    "    stopwords_wc.update([\"I'm, im, I am, i'm\"])\n",
    "\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    # lower max_font_size, change the maximum number of word and lighten the background:\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=20, background_color=\"white\").generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    print(f\"Cluster {i}: {comments_n} comments in this cluster. \\n\")\n",
    "    print (\"There are {} words in the combination of all comments in this cluster. \\n\".format(len(text)))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    # Save the image in the img folder:\n",
    "    # wordcloud.to_file(\"img/blah.png\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word clouds provide useful summaries of the cluster themes as summarised by a human looking at them earlier in the doc. Some repetition of terms, we should work out how to use  our stopwords list from earlier and the tokenized and stemmed comments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional sca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:16:07.675574Z",
     "start_time": "2020-04-11T14:14:46.289551Z"
    }
   },
   "outputs": [],
   "source": [
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing document clusters\n",
    "In this section, we demonstrate how you can visualize the document clustering output using matplotlib.\n",
    "\n",
    "First we define some dictionaries for going from cluster number to color and to cluster name. We based the cluster names off the words that were closest to each cluster centroid. Thus, it could be automated. See the earlier code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:44:51.023585Z",
     "start_time": "2020-04-11T14:44:51.017024Z"
    }
   },
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:41:06.586191Z",
     "start_time": "2020-04-11T14:41:06.582460Z"
    }
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Information, help, date', \n",
    "                 1: 'Vulnerable, person, register', \n",
    "                 2: 'Delivery, shops, slot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:47:29.625458Z",
     "start_time": "2020-04-11T14:47:29.301136Z"
    }
   },
   "outputs": [],
   "source": [
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df_mds = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df_mds.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the ... nothing relevant to label comments with\n",
    "# for i in range(len(df_mds)):\n",
    "#     ax.text(df_mds.iloc[i]['x'], df.iloc[i]['y'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T14:48:42.733606Z",
     "start_time": "2020-04-11T14:48:42.730844Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some overlap, also looks like a non-linear boundary would be better, could try t-sne (could also try different algos in place of k-means). This could also be a relic of comments being made up of sentences which describe different problems, thus it might make more sense to split up comments to the sentence level prior to clustering. The experimental unit could be at the level of sentence in a comment.\n",
    "\n",
    "This could be handled by LDA. LDA is a probabilistic topic model that assumes documents are a mixture of topics and that each word in the document is attributable to the document's topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "This section focuses on using Latent Dirichlet Allocation (LDA) to learn yet more about the hidden structure within the comments from users on the extremely vulnerable online form.\n",
    "\n",
    "This notebook is quite long, let's create a new one called `feedback_topic_modelling_extremely_vulnerable_people`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
